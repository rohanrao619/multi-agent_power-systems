{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40c5093",
   "metadata": {},
   "source": [
    "### Crude Evaluation for Sanity Check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e674ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env.energy_trading import EnergyTradingEnv\n",
    "\n",
    "from torch import Tensor\n",
    "from tensordict import TensorDict\n",
    "from benchmarl.experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_exp_path = \"results/mappo_simple_p2p_mlp__35a1cc94_25_08_21-22_11_46/checkpoints/checkpoint_131072.pt\"\n",
    "base_exp = Experiment.reload_from_file(base_exp_path)\n",
    "\n",
    "# Base environment\n",
    "config = base_exp.task.config\n",
    "trading_env = EnergyTradingEnv(config, render_mode=None)\n",
    "\n",
    "# Agent IDs\n",
    "agents = base_exp.group_map['agents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b680a",
   "metadata": {},
   "source": [
    "#### Select Policy for the Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply policy to trade, assuming single group\n",
    "def _trade_forward(obs):\n",
    "\n",
    "    stacked_obs = Tensor([obs[aid] for aid in base_exp.group_map['agents']])\n",
    "    obs_tdict = TensorDict(agents=TensorDict(observation=stacked_obs))\n",
    "\n",
    "    actions = base_exp.policy.forward(obs_tdict)['agents']['action'].detach().cpu().numpy()\n",
    "    action_dict = {aid: actions[i] for i, aid in enumerate(base_exp.group_map['agents'])}\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply policy to trade, assuming individual groups\n",
    "def _trade_forward(obs):\n",
    "    \n",
    "    obs_tdict = TensorDict({aid: TensorDict(observation=Tensor([obs[aid]])) for aid in agents})\n",
    "\n",
    "    actions = base_exp.policy.forward(obs_tdict)\n",
    "    action_dict = {aid: actions[aid]['action'].detach().cpu().numpy()[0] for aid in agents}\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5493893",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {\"0.08\":2,\n",
    "      \"0.13\":1,\n",
    "      \"0.18\":0}\n",
    "\n",
    "# Baseline heuristic, no RL\n",
    "def _trade_forward(obs):\n",
    "\n",
    "    ToU = str(obs[\"consumer_1\"][-2])\n",
    "    action_dict = {aid: pi[ToU] for aid in base_exp.group_map['agents']}\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f28bb",
   "metadata": {},
   "source": [
    "#### Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28573a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_average(x, agents, y_axis_label, plot_std=True):\n",
    "\n",
    "    x_mean = {aid: np.mean(np.array(x[aid]), axis=0) for aid in agents}\n",
    "    x_std = {aid: np.std(np.array(x[aid]), axis=0) for aid in agents}\n",
    "\n",
    "    for aid in agents:\n",
    "        \n",
    "        mean_vals = x_mean[aid]\n",
    "        plt.plot(mean_vals, label=aid)\n",
    "\n",
    "        if plot_std:\n",
    "            std_vals = x_std[aid]\n",
    "            plt.fill_between(\n",
    "                range(len(mean_vals)),\n",
    "                mean_vals - std_vals,\n",
    "                mean_vals + std_vals,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "    timesteps = range(config['eps_len'])\n",
    "    hours = [f\"{h}:00\" for h in timesteps]\n",
    "    \n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.xticks(timesteps, hours, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8e3a3",
   "metadata": {},
   "source": [
    "#### Rollout on Entire Dataset, Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bbd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logs for all agents and days\n",
    "log_keys = ['soc_action_log', 'price_action_log', 'soc_log', 'demand_log', 'p2p_log', 'p2p_price_log', 'reward_log']\n",
    "logs = {key: {aid: [[] for _ in range(trading_env.n_days-1)] for aid in agents} for key in log_keys}\n",
    "\n",
    "for day in tqdm(range(trading_env.n_days-1), desc=\"Trading Days\"):\n",
    "    \n",
    "    obs, infos = trading_env.reset(seed=42, options={\"day\": day})\n",
    "\n",
    "    # Step environment for an episode\n",
    "    for _ in range(trading_env.eps_len):\n",
    "\n",
    "        # Environment actor\n",
    "        actions = _trade_forward(obs)\n",
    "\n",
    "        for aid in agents:\n",
    "            \n",
    "            logs['demand_log'][aid][day].append(obs[aid][0].item())\n",
    "            logs['soc_log'][aid][day].append(obs[aid][1].item())\n",
    "            \n",
    "            logs['soc_action_log'][aid][day].append(actions[aid])\n",
    "            # logs['soc_action_log'][aid][day].append(actions[aid][1].item())\n",
    "            \n",
    "            # logs['price_action_log'][aid][day].append(actions[aid][0].item())\n",
    "\n",
    "        obs, rewards, terminations, truncations, infos = trading_env.step(actions)\n",
    "\n",
    "        # matches, trades, open_book = trading_env._run_double_auction(trading_env.orderbook)\n",
    "\n",
    "        for aid in agents:\n",
    "            # logs['p2p_log'][aid][day].append(trades[aid][\"qnt\"])\n",
    "            # logs['p2p_price_log'][aid][day].append(trades[aid][\"price\"])\n",
    "            logs['reward_log'][aid][day].append(rewards[aid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793788db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avergage rewards across agents, and all days\n",
    "reward_np = {aid: np.mean(np.sum(logs['reward_log'][aid],axis=-1)) for aid in agents}\n",
    "total_reward = np.mean(list(reward_np.values()))\n",
    "print(f\"Total reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plot_daily_average(logs['soc_action_log'], agents, 'State of Charge', plot_std=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855665ca",
   "metadata": {},
   "source": [
    "#### Deep Dive: Specif Days or Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 324\n",
    "agent = \"consumer_3\"\n",
    "    \n",
    "obs, infos = trading_env.reset(seed=42, options={\"day\": day})\n",
    "\n",
    "# Step environment for an episode\n",
    "for _ in range(trading_env.eps_len):\n",
    "\n",
    "    print(f\"\\nHour: {_:02}\")\n",
    "    print(f\"Observation: {obs[agent]}\")\n",
    "\n",
    "    # Environment actor\n",
    "    actions = _trade_forward(obs)\n",
    "\n",
    "    obs, rewards, terminations, truncations, infos = trading_env.step(actions)\n",
    "    \n",
    "    print(f\"Action: {actions[agent]}\")\n",
    "    print(f\"Reward: {rewards[agent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e86c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "x = deepcopy(logs['soc_action_log'])\n",
    "x[\"consumer_3\"] = x[\"consumer_3\"][:31] # July 2010\n",
    "plot_daily_average(x, [\"consumer_3\"], 'State of Charge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cfb3e",
   "metadata": {},
   "source": [
    "#### Critic Evaluation: Q and V Functions\n",
    "\n",
    "Currently only tested for MADDPG, single agent environment, only soc action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ToU = 0.08\n",
    "FiT = 0.04\n",
    "t = 0\n",
    "load = 0\n",
    "soc = 0\n",
    "soc_action = 0\n",
    "\n",
    "obs = {\"consumer_1\": [load, soc, ToU, FiT, np.sin(2*np.pi*t/24), np.cos(2*np.pi*t/24)]}\n",
    "action = {\"consumer_1\": [soc_action]}\n",
    "\n",
    "stacked_obs = Tensor([obs[aid] for aid in base_exp.group_map['agents']])\n",
    "stacked_action = Tensor([action[aid] for aid in base_exp.group_map['agents']])\n",
    "obs_tdict = TensorDict(agents=TensorDict(observation=stacked_obs,action=stacked_action))\n",
    "\n",
    "# Check if Q value makes sense?\n",
    "print(base_exp.losses[\"agents\"].value_network[0].forward(obs_tdict)['agents']['state_action_value'].cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some samples from replay buffer, sanity check\n",
    "x = base_exp.replay_buffers['agents'].sample(256)\n",
    "\n",
    "observations = x['agents']['observation']\n",
    "actions = x['agents']['action']\n",
    "episode_rewards = x['agents']['episode_reward']\n",
    "rewards = x['next']['agents']['reward']\n",
    "param = x['agents']['param']\n",
    "\n",
    "next_obs = x['next']['agents']['observation']\n",
    "next_episode_rewards = x['next']['agents']['episode_reward']\n",
    "terminated = x['next']['agents']['done']\n",
    "\n",
    "for i in range(256):\n",
    "    \n",
    "    obs = observations[i][0]\n",
    "    next_obs_val = next_obs[i][0]\n",
    "    terminated_val = terminated[i][0]\n",
    "    reward = rewards[i][0]\n",
    "    action = actions[i][0]\n",
    "    episode_reward = episode_rewards[i][0]\n",
    "    next_episode_reward = next_episode_rewards[i][0]\n",
    "    param_val = param[i][0]\n",
    "\n",
    "    print(f\"Observation: {obs}\\nAction: {action}\\nReward: {reward}\\nEpisode Reward: {episode_reward}\\nNext Observation: {next_obs_val}\\nNext Episode Reward: {next_episode_reward}\\nTerminated: {terminated_val}\\nParam: {param_val}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
