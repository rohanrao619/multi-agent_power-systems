{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40c5093",
   "metadata": {},
   "source": [
    "### Crude Evaluation for Sanity Check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e674ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env.energy_trading import EnergyTradingEnv\n",
    "\n",
    "from torch import Tensor\n",
    "from tensordict import TensorDict\n",
    "from benchmarl.experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_exp_path = \"results/da/mappo/seed_0/checkpoints/checkpoint_524288.pt\"\n",
    "base_exp = Experiment.reload_from_file(base_exp_path)\n",
    "\n",
    "# Base environment\n",
    "config = base_exp.task.config\n",
    "trading_env = EnergyTradingEnv(config, render_mode=None)\n",
    "\n",
    "# Agent IDs\n",
    "agents = base_exp.group_map['agents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce50fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AID Mapping\n",
    "# Set the standard in thesis-results/utils.py for consistency!\n",
    "print(trading_env.aid_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b680a",
   "metadata": {},
   "source": [
    "#### Select Policy for the Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Learner NN Policy to Trade\n",
    "\n",
    "def _trade_forward(obs):\n",
    "\n",
    "    stacked_obs = Tensor(np.array([obs[aid] for aid in base_exp.group_map['agents']], dtype=np.float32))\n",
    "    obs_tdict = TensorDict(agents=TensorDict(observation=stacked_obs)).to(base_exp.policy.device)\n",
    "\n",
    "    actions = base_exp.policy.forward(obs_tdict)['agents']['action'].detach().cpu().numpy()\n",
    "    action_dict = {aid: actions[i] for i, aid in enumerate(base_exp.group_map['agents'])}\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5493893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline heuristic, no RL\n",
    "# For example, always buy when cheap, sell when expensive\n",
    "\n",
    "pi = {\"0.15\":1,\n",
    "      \"0.22\":0,\n",
    "      \"0.44\":-1}\n",
    "\n",
    "def _trade_forward(obs):\n",
    "\n",
    "    ToU = str(obs[\"consumer_1\"][3])\n",
    "    action_dict = {aid: np.array([0.5, pi[ToU]]) for aid in base_exp.group_map['agents']} # For DA/MIX\n",
    "    # action_dict = {aid: np.array([pi[ToU]]) for aid in base_exp.group_map['agents']} # For Pool/Grid\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f28bb",
   "metadata": {},
   "source": [
    "#### Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28573a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_average(x, agents, y_axis_label, plot_std=True):\n",
    "\n",
    "    x_mean = {aid: np.mean(np.array(x[aid]), axis=0) for aid in agents}\n",
    "    x_std = {aid: np.std(np.array(x[aid]), axis=0) for aid in agents}\n",
    "\n",
    "    for aid in agents:\n",
    "        \n",
    "        mean_vals = x_mean[aid]\n",
    "        plt.plot(mean_vals, label=aid)\n",
    "\n",
    "        if plot_std:\n",
    "            std_vals = x_std[aid]\n",
    "            plt.fill_between(\n",
    "                range(len(mean_vals)),\n",
    "                mean_vals - std_vals,\n",
    "                mean_vals + std_vals,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "    timesteps = range(config['eps_len'])\n",
    "    hours = [f\"{h}:00\" for h in timesteps]\n",
    "    \n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.xticks(timesteps, hours, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8e3a3",
   "metadata": {},
   "source": [
    "#### Rollout on Entire Dataset, Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bbd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logs for all agents and days\n",
    "log_keys = ['soc_action_log', 'price_action_log',\n",
    "            'soc_log', 'demand_log', 'reward_log',\n",
    "            'da_vol_log', 'da_price_log']\n",
    "\n",
    "logs = {key: {aid: [[] for _ in range(1095)] for aid in agents} for key in log_keys}\n",
    "\n",
    "# Specify init state\n",
    "soc = {aid: 4.0 for aid in agents}\n",
    "contracts = list({aid: 0 for aid in agents} for _ in range(24)) # TODO: Get optimal contracts for MIX (see results.ipynb)\n",
    "\n",
    "for day in tqdm(range(1095), desc=\"Trading Days\"):\n",
    "    \n",
    "    obs, infos = trading_env.reset(seed=42, options={\"day\": day,\n",
    "                                                     \"soc\": deepcopy(soc),\n",
    "                                                     \"contracts\": contracts}) # Only in force when use_contracts=True\n",
    "\n",
    "    # Step environment for an episode\n",
    "    for t in range(trading_env.eps_len):\n",
    "\n",
    "        # Environment actor\n",
    "        actions = _trade_forward(obs)\n",
    "\n",
    "        for aid in agents:\n",
    "            \n",
    "            logs['demand_log'][aid][day].append(obs[aid][0].item())\n",
    "            logs['soc_log'][aid][day].append(obs[aid][2].item()*8) # TODO: Fix hardcoded max soc            \n",
    "            \n",
    "            # logs['soc_action_log'][aid][day].append(actions[aid][0].item()) # For Pool/Grid\n",
    "            logs['soc_action_log'][aid][day].append(actions[aid][1].item()) # For DA/MIX\n",
    "            \n",
    "            logs['price_action_log'][aid][day].append(actions[aid][0].item()) # Only for DA/MIX\n",
    "\n",
    "        obs, rewards, terminations, truncations, infos = trading_env.step(actions)\n",
    "\n",
    "        matches, trades, open_book = trading_env._run_double_auction(trading_env.orderbook) # Only for DA/MIX\n",
    "\n",
    "        for aid in agents:\n",
    "            logs['da_vol_log'][aid][day].append(trades[aid][\"qnt\"]) # Only for DA/MIX\n",
    "            logs['da_price_log'][aid][day].append(trades[aid][\"price\"]) # Only for DA/MIX\n",
    "            logs['reward_log'][aid][day].append(rewards[aid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793788db",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reward = {aid: np.sum(logs['reward_log'][aid], axis=-1) for aid in agents}\n",
    "\n",
    "# Avergage rewards across agents, and all days\n",
    "average_reward = np.mean(list(daily_reward.values()))\n",
    "print(f\"Communal daily average reward: {average_reward:.3f}\")\n",
    "\n",
    "# Total communal cost\n",
    "total_reward = np.sum(list(daily_reward.values()))\n",
    "print(f\"Total communal cost: {total_reward:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70acc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution among Community\n",
    "for agent, reward in daily_reward.items():\n",
    "    print(f\"{agent}: {np.mean(reward):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Stuff\n",
    "plot_daily_average(logs['soc_log'], agents, 'SoC', plot_std=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855665ca",
   "metadata": {},
   "source": [
    "#### Deep Dive: Specific Days or Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 42\n",
    "agent = \"consumer_4\"\n",
    "\n",
    "# Specify init state\n",
    "soc = {aid: 4.0 for aid in agents}\n",
    "contracts = list({aid: 0.0 for aid in agents} for _ in range(24)) # TODO: Get optimal contracts for MIX (see results.ipynb)\n",
    "    \n",
    "obs, infos = trading_env.reset(seed=42, options={\"day\": day,\n",
    "                                                 \"soc\": deepcopy(soc),\n",
    "                                                 \"contracts\": contracts}) # Only in force when use_contracts=True\n",
    "\n",
    "# Step environment for an episode\n",
    "for _ in range(trading_env.eps_len):\n",
    "\n",
    "    print(f\"\\nHour: {_:02}\")\n",
    "    print(f\"Observation: {obs[agent]}\")\n",
    "\n",
    "    # Environment actor\n",
    "    actions = _trade_forward(obs)\n",
    "\n",
    "    obs, rewards, terminations, truncations, infos = trading_env.step(actions)\n",
    "    \n",
    "    print(f\"Action: {actions[agent]}\")\n",
    "    print(f\"Reward: {rewards[agent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e86c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Specifics\n",
    "x = deepcopy(logs['demand_log'])\n",
    "x[\"consumer_4\"] = x[\"consumer_4\"][:31] # July 2010\n",
    "plot_daily_average(x, [\"consumer_4\"], 'Demand')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cfb3e",
   "metadata": {},
   "source": [
    "#### Critic Evaluation: Q and V Functions [Deprecated]\n",
    "\n",
    "Currently only tested for MADDPG, single agent environment, only soc action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ToU = 0.08\n",
    "FiT = 0.04\n",
    "t = 0\n",
    "load = 0\n",
    "soc = 0\n",
    "soc_action = 0\n",
    "\n",
    "obs = {\"consumer_1\": [load, soc, ToU, FiT, np.sin(2*np.pi*t/24), np.cos(2*np.pi*t/24)]}\n",
    "action = {\"consumer_1\": [soc_action]}\n",
    "\n",
    "stacked_obs = Tensor([obs[aid] for aid in base_exp.group_map['agents']])\n",
    "stacked_action = Tensor([action[aid] for aid in base_exp.group_map['agents']])\n",
    "obs_tdict = TensorDict(agents=TensorDict(observation=stacked_obs,action=stacked_action))\n",
    "\n",
    "# Check if Q value makes sense?\n",
    "print(base_exp.losses[\"agents\"].value_network[0].forward(obs_tdict)['agents']['state_action_value'].cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some samples from replay buffer, sanity check\n",
    "x = base_exp.replay_buffers['agents'].sample(256)\n",
    "\n",
    "observations = x['agents']['observation']\n",
    "actions = x['agents']['action']\n",
    "episode_rewards = x['agents']['episode_reward']\n",
    "rewards = x['next']['agents']['reward']\n",
    "param = x['agents']['param']\n",
    "\n",
    "next_obs = x['next']['agents']['observation']\n",
    "next_episode_rewards = x['next']['agents']['episode_reward']\n",
    "terminated = x['next']['agents']['done']\n",
    "\n",
    "for i in range(256):\n",
    "    \n",
    "    obs = observations[i][0]\n",
    "    next_obs_val = next_obs[i][0]\n",
    "    terminated_val = terminated[i][0]\n",
    "    reward = rewards[i][0]\n",
    "    action = actions[i][0]\n",
    "    episode_reward = episode_rewards[i][0]\n",
    "    next_episode_reward = next_episode_rewards[i][0]\n",
    "    param_val = param[i][0]\n",
    "\n",
    "    print(f\"Observation: {obs}\\nAction: {action}\\nReward: {reward}\\nEpisode Reward: {episode_reward}\\nNext Observation: {next_obs_val}\\nNext Episode Reward: {next_episode_reward}\\nTerminated: {terminated_val}\\nParam: {param_val}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
