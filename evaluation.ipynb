{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40c5093",
   "metadata": {},
   "source": [
    "### Crude Evaluation for Sanity Check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e674ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env.energy_trading import EnergyTradingEnv\n",
    "\n",
    "from torch import Tensor\n",
    "from tensordict import TensorDict\n",
    "from benchmarl.experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_exp_path = \"results/mappo_simple_p2p_mlp__82af9d50_25_09_04-22_30_47/checkpoints/checkpoint_524288.pt\"\n",
    "base_exp = Experiment.reload_from_file(base_exp_path)\n",
    "\n",
    "# Base environment\n",
    "config = base_exp.task.config\n",
    "trading_env = EnergyTradingEnv(config, render_mode=None)\n",
    "\n",
    "# Agent IDs\n",
    "agents = base_exp.group_map['agents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cffc9e",
   "metadata": {},
   "source": [
    "#### Get 2nd Stage Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_exp_path = \"results/mappo_hourly_commits_mlp__0444c703_25_09_04-23_29_51/checkpoints/checkpoint_393216.pt\"\n",
    "base_exp = Experiment.reload_from_file(base_exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts =  list(dict() for _ in range(24))\n",
    "\n",
    "for i in range(24):\n",
    "\n",
    "    t = i / 24.0\n",
    "\n",
    "    obs = [np.sin(2 * np.pi * t),\n",
    "        np.cos(2 * np.pi * t)]\n",
    "\n",
    "    stacked_obs = Tensor([obs for aid in base_exp.group_map['agents']])\n",
    "    obs_tdict = TensorDict(agents=TensorDict(observation=stacked_obs)).to(base_exp.policy.device)\n",
    "\n",
    "    actions = base_exp.policy.forward(obs_tdict)['agents']['action'].detach().cpu().numpy()\n",
    "    action_dict = {aid: actions[i] for i, aid in enumerate(base_exp.group_map['agents'])}\n",
    "\n",
    "    for aid in base_exp.group_map['agents']:\n",
    "        contracts[i][aid] = action_dict[aid][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save contracts to a file\n",
    "with open(\"contracts.pkl\", \"wb\") as file:\n",
    "    pickle.dump(contracts, file)\n",
    "\n",
    "print(\"Contracts saved to 'contracts.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155522b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot contracts for each agent\n",
    "for aid in agents:\n",
    "    contract_values = [contracts[t][aid] for t in range(len(contracts))]\n",
    "    plt.plot(contract_values, label=aid)\n",
    "\n",
    "timesteps = range(config['eps_len'])\n",
    "hours = [f\"{h}:00\" for h in timesteps]\n",
    "\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Contract Quantity')\n",
    "plt.xticks(timesteps, hours, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b680a",
   "metadata": {},
   "source": [
    "#### Select Policy for the Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply policy to trade, assuming single group\n",
    "def _trade_forward(obs):\n",
    "\n",
    "    stacked_obs = Tensor([obs[aid] for aid in base_exp.group_map['agents']])\n",
    "    obs_tdict = TensorDict(agents=TensorDict(observation=stacked_obs)).to(base_exp.policy.device)\n",
    "\n",
    "    actions = base_exp.policy.forward(obs_tdict)['agents']['action'].detach().cpu().numpy()\n",
    "    action_dict = {aid: actions[i] for i, aid in enumerate(base_exp.group_map['agents'])}\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply policy to trade, assuming individual groups\n",
    "def _trade_forward(obs):\n",
    "    \n",
    "    obs_tdict = TensorDict({aid: TensorDict(observation=Tensor([obs[aid]])) for aid in agents}).to(base_exp.policy.device)\n",
    "\n",
    "    actions = base_exp.policy.forward(obs_tdict)\n",
    "    action_dict = {aid: actions[aid]['action'].detach().cpu().numpy()[0] for aid in agents}\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5493893",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {\"0.08\":1,\n",
    "      \"0.13\":0,\n",
    "      \"0.18\":-1}\n",
    "\n",
    "# Baseline heuristic, no RL\n",
    "def _trade_forward(obs):\n",
    "\n",
    "    ToU = str(obs[\"consumer_1\"][-2])\n",
    "    action_dict = {aid: pi[ToU] for aid in base_exp.group_map['agents']}\n",
    "    \n",
    "    return action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f28bb",
   "metadata": {},
   "source": [
    "#### Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28573a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_average(x, agents, y_axis_label, plot_std=True):\n",
    "\n",
    "    x_mean = {aid: np.mean(np.array(x[aid]), axis=0) for aid in agents}\n",
    "    x_std = {aid: np.std(np.array(x[aid]), axis=0) for aid in agents}\n",
    "\n",
    "    for aid in agents:\n",
    "        \n",
    "        mean_vals = x_mean[aid]\n",
    "        plt.plot(mean_vals, label=aid)\n",
    "\n",
    "        if plot_std:\n",
    "            std_vals = x_std[aid]\n",
    "            plt.fill_between(\n",
    "                range(len(mean_vals)),\n",
    "                mean_vals - std_vals,\n",
    "                mean_vals + std_vals,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "    timesteps = range(config['eps_len'])\n",
    "    hours = [f\"{h}:00\" for h in timesteps]\n",
    "    \n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.xticks(timesteps, hours, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8e3a3",
   "metadata": {},
   "source": [
    "#### Rollout on Entire Dataset, Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bbd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logs for all agents and days\n",
    "log_keys = ['soc_action_log', 'price_action_log',\n",
    "            'soc_log', 'demand_log', 'reward_log', 'grid_reliance_log',\n",
    "            'p2p_log', 'p2p_price_log']\n",
    "\n",
    "logs = {key: {aid: [[] for _ in range(trading_env.n_days-1)] for aid in agents} for key in log_keys}\n",
    "\n",
    "# Specify init state\n",
    "soc = {aid: 1.0 for aid in agents}\n",
    "# contracts = list({aid: 0.0 for aid in agents} for _ in range(24))\n",
    "\n",
    "for day in tqdm(range(trading_env.n_days-1), desc=\"Trading Days\"):\n",
    "    \n",
    "    obs, infos = trading_env.reset(seed=42, options={\"day\": day,\n",
    "                                                     \"soc\": deepcopy(soc),\n",
    "                                                     \"contracts\": contracts})\n",
    "\n",
    "    # Step environment for an episode\n",
    "    for t in range(trading_env.eps_len):\n",
    "\n",
    "        # Environment actor\n",
    "        actions = _trade_forward(obs)\n",
    "\n",
    "        for aid in agents:\n",
    "            \n",
    "            logs['demand_log'][aid][day].append(obs[aid][0].item())\n",
    "            logs['soc_log'][aid][day].append(obs[aid][1].item())            \n",
    "            \n",
    "            # logs['soc_action_log'][aid][day].append(actions[aid][0].item())\n",
    "            logs['soc_action_log'][aid][day].append(actions[aid][1].item())\n",
    "            \n",
    "            logs['price_action_log'][aid][day].append(actions[aid][0].item())\n",
    "\n",
    "        obs, rewards, terminations, truncations, infos = trading_env.step(actions)\n",
    "\n",
    "        matches, trades, open_book = trading_env._run_double_auction(trading_env.orderbook)\n",
    "\n",
    "        for aid in agents:\n",
    "            logs['p2p_log'][aid][day].append(trades[aid][\"qnt\"])\n",
    "            logs['p2p_price_log'][aid][day].append(trades[aid][\"price\"])\n",
    "            logs['reward_log'][aid][day].append(rewards[aid])\n",
    "\n",
    "            logs['grid_reliance_log'][aid][day].append(trading_env.state_vars[aid]['grid_reliance'])\n",
    "            if t != 0:\n",
    "                logs['grid_reliance_log'][aid][day][t] -= logs['grid_reliance_log'][aid][day][t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793788db",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reward = {aid: np.sum(logs['reward_log'][aid], axis=-1) for aid in agents}\n",
    "\n",
    "# Avergage rewards across agents, and all days\n",
    "average_reward = np.mean(list(daily_reward.values()))\n",
    "print(f\"Communal daily average reward: {average_reward:.3f}\")\n",
    "\n",
    "# Total communal cost\n",
    "total_reward = np.sum(list(daily_reward.values()))\n",
    "print(f\"Total communal cost: {total_reward:.2f} $\")\n",
    "\n",
    "# Total grid reliance\n",
    "total_reliance = np.sum(list(logs['grid_reliance_log'].values()))\n",
    "# total_reliance = np.sum(logs['grid_reliance_log']['consumer_1'])\n",
    "print(f\"Total grid reliance: {total_reliance:.2f} kWh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70acc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent, reward in daily_reward.items():\n",
    "    print(f\"{agent}: {np.mean(reward):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plot_daily_average(logs['soc_log'], agents, 'SoC', plot_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855665ca",
   "metadata": {},
   "source": [
    "#### Deep Dive: Specific Days or Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 42\n",
    "agent = \"consumer_4\"\n",
    "\n",
    "# Specify init state\n",
    "soc = {aid: 1.0 for aid in agents}\n",
    "contracts = list({aid: 0.0 for aid in agents} for _ in range(24))\n",
    "    \n",
    "obs, infos = trading_env.reset(seed=42, options={\"day\": day,\n",
    "                                                 \"soc\": deepcopy(soc),\n",
    "                                                 \"contracts\": contracts})\n",
    "\n",
    "# Step environment for an episode\n",
    "for _ in range(trading_env.eps_len):\n",
    "\n",
    "    print(f\"\\nHour: {_:02}\")\n",
    "    print(f\"Observation: {obs[agent]}\")\n",
    "\n",
    "    # Environment actor\n",
    "    actions = _trade_forward(obs)\n",
    "\n",
    "    obs, rewards, terminations, truncations, infos = trading_env.step(actions)\n",
    "    \n",
    "    print(f\"Action: {actions[agent]}\")\n",
    "    print(f\"Reward: {rewards[agent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e86c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "x = deepcopy(logs['demand_log'])\n",
    "x[\"consumer_4\"] = x[\"consumer_4\"][:31] # July 2010\n",
    "plot_daily_average(x, [\"consumer_4\"], 'Demand')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cfb3e",
   "metadata": {},
   "source": [
    "#### Critic Evaluation: Q and V Functions\n",
    "\n",
    "Currently only tested for MADDPG, single agent environment, only soc action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ToU = 0.08\n",
    "FiT = 0.04\n",
    "t = 0\n",
    "load = 0\n",
    "soc = 0\n",
    "soc_action = 0\n",
    "\n",
    "obs = {\"consumer_1\": [load, soc, ToU, FiT, np.sin(2*np.pi*t/24), np.cos(2*np.pi*t/24)]}\n",
    "action = {\"consumer_1\": [soc_action]}\n",
    "\n",
    "stacked_obs = Tensor([obs[aid] for aid in base_exp.group_map['agents']])\n",
    "stacked_action = Tensor([action[aid] for aid in base_exp.group_map['agents']])\n",
    "obs_tdict = TensorDict(agents=TensorDict(observation=stacked_obs,action=stacked_action))\n",
    "\n",
    "# Check if Q value makes sense?\n",
    "print(base_exp.losses[\"agents\"].value_network[0].forward(obs_tdict)['agents']['state_action_value'].cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some samples from replay buffer, sanity check\n",
    "x = base_exp.replay_buffers['agents'].sample(256)\n",
    "\n",
    "observations = x['agents']['observation']\n",
    "actions = x['agents']['action']\n",
    "episode_rewards = x['agents']['episode_reward']\n",
    "rewards = x['next']['agents']['reward']\n",
    "param = x['agents']['param']\n",
    "\n",
    "next_obs = x['next']['agents']['observation']\n",
    "next_episode_rewards = x['next']['agents']['episode_reward']\n",
    "terminated = x['next']['agents']['done']\n",
    "\n",
    "for i in range(256):\n",
    "    \n",
    "    obs = observations[i][0]\n",
    "    next_obs_val = next_obs[i][0]\n",
    "    terminated_val = terminated[i][0]\n",
    "    reward = rewards[i][0]\n",
    "    action = actions[i][0]\n",
    "    episode_reward = episode_rewards[i][0]\n",
    "    next_episode_reward = next_episode_rewards[i][0]\n",
    "    param_val = param[i][0]\n",
    "\n",
    "    print(f\"Observation: {obs}\\nAction: {action}\\nReward: {reward}\\nEpisode Reward: {episode_reward}\\nNext Observation: {next_obs_val}\\nNext Episode Reward: {next_episode_reward}\\nTerminated: {terminated_val}\\nParam: {param_val}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
